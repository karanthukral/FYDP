{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/code/FYDP/venv/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from os import path\n",
    "\n",
    "from ngram import NGram \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "bro_file = path.join(DATA_DIR, \"bro.dat\")\n",
    "good_urls_file = path.join(DATA_DIR, \"top-1m.csv\")\n",
    "\n",
    "DOMAIN_MATCH = 'Intel::DOMAIN'\n",
    "\n",
    "NGRAM_N = 3\n",
    "MAX_DOMAINS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bro\n",
    "\n",
    "## Parse bro list of bad domains, IPs, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 32 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bad_domains = []\n",
    "good_domains = []\n",
    "\n",
    "with open(bro_file) as f:\n",
    "    f.readline() # first line is a comment. skip over it now\n",
    "    for idx, line in enumerate(f):\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        if len(l) is not 4:\n",
    "            continue\n",
    "\n",
    "        if l[1] == DOMAIN_MATCH and len(bad_domains) < MAX_DOMAINS:\n",
    "            bad_domains.append(l[0].lower())\n",
    "\n",
    "with open(good_urls_file) as f:\n",
    "    good_domains = [line.rstrip().split(\",\")[1].lower() for idx, line in enumerate(f) if idx < MAX_DOMAINS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML\n",
    "\n",
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_gen = NGram(N=NGRAM_N)\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyz1234567890.-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_perm = [\"\".join(tup) for tup in itertools.product(char_list, repeat=NGRAM_N)]\n",
    "perm_lookup = {perm: idx for idx, perm in enumerate(n_perm)}\n",
    "feature_length = len(perm_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data vomiter\n",
    "class data_vomit():\n",
    "        \n",
    "    def __init__(self):\n",
    "        \n",
    "        class testing():\n",
    "            def __init__(self):\n",
    "                self.domains = []\n",
    "                self.labels = []\n",
    "        \n",
    "        self.test = testing()\n",
    "        self.data_feed = np.array([[d, 1] for d in good_domains] + [[d, 0] for d in bad_domains])\n",
    "        np.random.shuffle(self.data_feed)\n",
    "        self.X_train, self.test.domains, self.y_train, self.test.labels = train_test_split(self.data_feed[:,0], self.data_feed[:,1], test_size=0.03)\n",
    "\n",
    "        self.test.labels = self.to_one_hot(self.test.labels)\n",
    "        self.y_train = self.to_one_hot(self.y_train)\n",
    "\n",
    "        self.loc = 0\n",
    "        self.total_data = len(self.X_train)\n",
    "        \n",
    "    def str2ngram(self, domains_arr):\n",
    "        ngram_result = []\n",
    "        for d in domains_arr:\n",
    "            gram_result = np.zeros(feature_length)\n",
    "            domain = \"\".join([c for c in list(d) if c in char_list])\n",
    "            for gram in list(ngram_gen._split(d)):\n",
    "                gram_result[perm_lookup[gram]] += 1\n",
    "            ngram_result.append(gram_result)\n",
    "        return ngram_result\n",
    "        \n",
    "    def to_one_hot(self, arr):\n",
    "        arr = np.array(arr, dtype=np.int8)\n",
    "        result = np.zeros((len(arr), 2))\n",
    "        result[np.arange(len(arr)), arr] = 1\n",
    "        return result\n",
    "            \n",
    "    def next_batch(self, n):\n",
    "        if n + self.loc <= self.total_data:\n",
    "            x = self.X_train[self.loc:self.loc+n]\n",
    "            y = self.y_train[self.loc:self.loc+n]\n",
    "            self.loc = (self.loc + n) % self.total_data\n",
    "            return x, y\n",
    "        else:\n",
    "            x = np.append(self.X_train[self.loc:self.total_data],self.X_train[0:n-self.total_data+self.loc])\n",
    "            y = np.append(self.y_train[self.loc:self.total_data],self.y_train[0:n-self.total_data+self.loc])\n",
    "            self.loc = n-self.total_data+self.loc\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, feature_length])\n",
    "W1 = tf.Variable(tf.random_normal([feature_length, 1000]))\n",
    "b1 = tf.Variable(tf.random_normal([1000]))\n",
    "hl1 = tf.matmul(x, W1) + b1\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([1000, 500]))\n",
    "b2 = tf.Variable(tf.random_normal([500]))\n",
    "hl2 = tf.matmul(hl1, W2) + b2\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([500, 2]))\n",
    "b3 = tf.Variable(tf.random_normal([2]))\n",
    "y = tf.matmul(hl2, W3) + b3\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"correct_label\")\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 83.7333321571%\n",
      "Epoch: 2: 86.2333357334%\n",
      "CPU times: user 58min 43s, sys: 35.1 s, total: 59min 18s\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def evaluate_success(sess):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={x: data.str2ngram(data.test.domains), y_: data.test.labels}) * 100\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "MINIBATCH_SIZE = 200\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "data = data_vomit()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    for _ in range(int(feature_length / MINIBATCH_SIZE)):\n",
    "        batch_xs, batch_ys = data.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(train_step, feed_dict={x: data.str2ngram(batch_xs), y_: batch_ys})\n",
    "    print(\"Epoch: \" + str(i+1) + \": \" + str(evaluate_success(sess)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
