{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/code/FYDP/venv/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from os import path\n",
    "\n",
    "from ngram import NGram \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "bro_file = path.join(DATA_DIR, \"bro.dat\")\n",
    "good_urls_file = path.join(DATA_DIR, \"top-1m.csv\")\n",
    "\n",
    "DOMAIN_MATCH = 'Intel::DOMAIN'\n",
    "ADDR_MATCH = 'Intel::ADDR'\n",
    "URL_MATCH = 'Intel::URL'\n",
    "NGRAM_N = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bro\n",
    "\n",
    "## Parse bro list of bad domains, IPs, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 72 ms, total: 1.36 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bad_urls = []\n",
    "addrs = []\n",
    "domains = []\n",
    "unknowns = []\n",
    "\n",
    "with open(bro_file) as f:\n",
    "    f.readline() # first line is a comment. skip over it now\n",
    "    for idx, line in enumerate(f):\n",
    "        l = line.strip()\n",
    "            \n",
    "        l = l.split(\"\\t\")\n",
    "        if len(l) is not 4:\n",
    "            continue\n",
    "            \n",
    "        if l[1] == DOMAIN_MATCH:\n",
    "            domains.append(l[0])\n",
    "        elif l[1] == ADDR_MATCH:\n",
    "            addrs.append(l[0])\n",
    "        elif l[1] == URL_MATCH:\n",
    "            bad_urls.append(l[0])\n",
    "        else:\n",
    "            unknowns.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPs: 155313\n",
      "URLs: 44224\n",
      "Domains: 1182345\n",
      "Etc: 9256\n"
     ]
    }
   ],
   "source": [
    "print(\"IPs: \" + str(len(addrs)))\n",
    "print(\"URLs: \" + str(len(bad_urls)))\n",
    "print(\"Domains: \" + str(len(domains)))\n",
    "print(\"Etc: \" + str(len(unknowns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Intel::FILE_HASH', 'Intel::FILE_NAME', 'Intel::EMAIL'}\n"
     ]
    }
   ],
   "source": [
    "print(set([l[1] for l in unknowns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Begin by loading in the top 50k \"good\" domains. Generate n-grams for all domains, and combine in a manner suitable for sklearns classifiers. Do a train / test split at 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_gen = NGram(N=NGRAM_N)\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyz1234567890\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(good_urls_file) as f:\n",
    "    good_urls = [line.rstrip().split(\",\")[1] for idx, line in enumerate(f) if idx < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_perm = [\"\".join(tup) for tup in itertools.product(char_list, repeat=NGRAM_N)]\n",
    "perm_lookup = {perm: idx for idx, perm in enumerate(n_perm)}\n",
    "feature_length = len(perm_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_urls_features = np.zeros([len(good_urls), feature_length], dtype=np.int8)\n",
    "for idx, good_url in enumerate(good_urls):\n",
    "    good_url = \"\".join([c for c in list(good_url) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(good_url)):\n",
    "        good_urls_features[idx, perm_lookup[gram]] += 1\n",
    "        \n",
    "bad_urls_features = np.zeros([len(bad_urls), feature_length], dtype=np.int8)\n",
    "for idx, bad_url in enumerate(bad_urls):\n",
    "    bad_url = \"\".join([c for c in list(bad_url) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(bad_url)):\n",
    "        bad_urls_features[idx, perm_lookup[gram]] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = np.append(good_urls_features, bad_urls_features, axis=0), [\"good\" for i in range(len(good_urls_features))] + [\"bad\" for i in range(len(bad_urls_features))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.97317031754118 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.95      0.99      0.97     12456\n",
      "        bad       0.99      0.94      0.97     11100\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23556\n",
      "\n",
      "CPU times: user 1min 2s, sys: 152 ms, total: 1min 2s\n",
      "Wall time: 9.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, n_jobs=8)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.64459161147903 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.86      0.90      0.88     12456\n",
      "        bad       0.88      0.83      0.85     11100\n",
      "\n",
      "avg / total       0.87      0.87      0.87     23556\n",
      "\n",
      "CPU times: user 284 ms, sys: 72 ms, total: 356 ms\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = MultinomialNB(alpha=150)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
