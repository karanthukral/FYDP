{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/code/FYDP/venv/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from os import path\n",
    "\n",
    "from ngram import NGram \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "bro_file = path.join(DATA_DIR, \"bro.dat\")\n",
    "good_urls_file = path.join(DATA_DIR, \"top-1m.csv\")\n",
    "\n",
    "DOMAIN_MATCH = 'Intel::DOMAIN'\n",
    "ADDR_MATCH = 'Intel::ADDR'\n",
    "URL_MATCH = 'Intel::URL'\n",
    "\n",
    "CORES = multiprocessing.cpu_count()\n",
    "\n",
    "NGRAM_N = 2\n",
    "MAX_DOMAINS = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bro\n",
    "\n",
    "## Parse bro list of bad domains, IPs, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 56 ms, total: 1.34 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bad_urls = []\n",
    "bad_addrs = []\n",
    "bad_domains = []\n",
    "unknowns = []\n",
    "\n",
    "with open(bro_file) as f:\n",
    "    f.readline() # first line is a comment. skip over it now\n",
    "    for idx, line in enumerate(f):\n",
    "        l = line.strip()\n",
    "            \n",
    "        l = l.split(\"\\t\")\n",
    "        if len(l) is not 4:\n",
    "            continue\n",
    "            \n",
    "        if l[1] == DOMAIN_MATCH:\n",
    "            bad_domains.append(l[0])\n",
    "        elif l[1] == ADDR_MATCH:\n",
    "            bad_addrs.append(l[0])\n",
    "        elif l[1] == URL_MATCH:\n",
    "            bad_urls.append(l[0])\n",
    "        else:\n",
    "            unknowns.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPs: 155313\n",
      "URLs: 44224\n",
      "Domains: 1182345\n",
      "Etc: 9256\n"
     ]
    }
   ],
   "source": [
    "print(\"IPs: \" + str(len(bad_addrs)))\n",
    "print(\"URLs: \" + str(len(bad_urls)))\n",
    "print(\"Domains: \" + str(len(bad_domains)))\n",
    "print(\"Etc: \" + str(len(unknowns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Intel::EMAIL', 'Intel::FILE_HASH', 'Intel::FILE_NAME'}\n"
     ]
    }
   ],
   "source": [
    "print(set([l[1] for l in unknowns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Begin by loading in the top 50k \"good\" domains. Generate n-grams for all domains, and combine in a manner suitable for sklearns classifiers. Do a train / test split at 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_gen = NGram(N=NGRAM_N)\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyz1234567890.-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(good_urls_file) as f:\n",
    "    good_domains = [line.rstrip().split(\",\")[1] for idx, line in enumerate(f) if idx < MAX_DOMAINS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_perm = [\"\".join(tup) for tup in itertools.product(char_list, repeat=NGRAM_N)]\n",
    "perm_lookup = {perm: idx for idx, perm in enumerate(n_perm)}\n",
    "feature_length = len(perm_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take up to 50k bad domains\n",
    "if len(bad_domains) > MAX_DOMAINS:\n",
    "    bad_domains = bad_domains[0:MAX_DOMAINS]\n",
    "\n",
    "domains_features = np.zeros([len(good_domains) + len(bad_domains), feature_length], dtype=np.int8)\n",
    "\n",
    "for idx, good_domain in enumerate(good_domains):\n",
    "    good_url = \"\".join([c for c in list(good_domain) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(good_domain)):\n",
    "        domains_features[idx, perm_lookup[gram]] += 1\n",
    "        \n",
    "for idx, bad_url in enumerate(bad_domains):\n",
    "    bad_url = \"\".join([c for c in list(bad_url) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(bad_url)):\n",
    "        domains_features[idx + len(good_domains), perm_lookup[gram]] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [\"good\" for i in range(len(good_domains))] + [\"bad\" for i in range(len(bad_domains))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(domains_features, y, test_size=0.25)\n",
    "\n",
    "del(domains_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.096 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.80      0.91      0.85     12526\n",
      "        bad       0.90      0.77      0.83     12474\n",
      "\n",
      "avg / total       0.85      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def NB(X_train, X_test, y_train, y_test, alpha=150):\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Perform the predictions\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "    print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))\n",
    "\n",
    "#NB(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.27199999999999 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.85      0.97      0.91     12526\n",
      "        bad       0.97      0.83      0.90     12474\n",
      "\n",
      "avg / total       0.91      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def random_forest(trees, X_train, X_test, y_train, y_test, cores=multiprocessing.cpu_count()):\n",
    "    clf = RandomForestClassifier(n_estimators=trees, n_jobs=cores)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Perform the predictions\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "    print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))\n",
    "    \n",
    "#random_forest(40, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train, y_test, kernel='rbf'):\n",
    "    clf = SVC(kernel=kernel)\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Perform the predictions\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "    print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))\n",
    "\n",
    "#svm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data vomiter\n",
    "def single_epoch_data():\n",
    "    data_feed = [[d, \"good\"] for d in good_domains] + [[d, \"bad\"] for d in bad_domains]\n",
    "    random.shuffle(data_feed)\n",
    "    for datum in data_feed:\n",
    "        yield datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
