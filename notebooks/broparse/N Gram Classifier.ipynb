{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/code/FYDP/venv/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from os import path\n",
    "\n",
    "from ngram import NGram \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "bro_file = path.join(DATA_DIR, \"bro.dat\")\n",
    "good_urls_file = path.join(DATA_DIR, \"top-1m.csv\")\n",
    "\n",
    "DOMAIN_MATCH = 'Intel::DOMAIN'\n",
    "ADDR_MATCH = 'Intel::ADDR'\n",
    "URL_MATCH = 'Intel::URL'\n",
    "NGRAM_N = 2\n",
    "\n",
    "MAX_DOMAINS = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bro\n",
    "\n",
    "## Parse bro list of bad domains, IPs, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 56 ms, total: 1.32 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bad_urls = []\n",
    "bad_addrs = []\n",
    "bad_domains = []\n",
    "unknowns = []\n",
    "\n",
    "with open(bro_file) as f:\n",
    "    f.readline() # first line is a comment. skip over it now\n",
    "    for idx, line in enumerate(f):\n",
    "        l = line.strip()\n",
    "            \n",
    "        l = l.split(\"\\t\")\n",
    "        if len(l) is not 4:\n",
    "            continue\n",
    "            \n",
    "        if l[1] == DOMAIN_MATCH:\n",
    "            bad_domains.append(l[0])\n",
    "        elif l[1] == ADDR_MATCH:\n",
    "            bad_addrs.append(l[0])\n",
    "        elif l[1] == URL_MATCH:\n",
    "            bad_urls.append(l[0])\n",
    "        else:\n",
    "            unknowns.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPs: 155313\n",
      "URLs: 44224\n",
      "Domains: 1182345\n",
      "Etc: 9256\n"
     ]
    }
   ],
   "source": [
    "print(\"IPs: \" + str(len(bad_addrs)))\n",
    "print(\"URLs: \" + str(len(bad_urls)))\n",
    "print(\"Domains: \" + str(len(bad_domains)))\n",
    "print(\"Etc: \" + str(len(unknowns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Intel::FILE_HASH', 'Intel::EMAIL', 'Intel::FILE_NAME'}\n"
     ]
    }
   ],
   "source": [
    "print(set([l[1] for l in unknowns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Begin by loading in the top 50k \"good\" domains. Generate n-grams for all domains, and combine in a manner suitable for sklearns classifiers. Do a train / test split at 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_gen = NGram(N=NGRAM_N)\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyz1234567890.-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(good_urls_file) as f:\n",
    "    good_urls = [line.rstrip().split(\",\")[1] for idx, line in enumerate(f) if idx < MAX_DOMAINS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_perm = [\"\".join(tup) for tup in itertools.product(char_list, repeat=NGRAM_N)]\n",
    "perm_lookup = {perm: idx for idx, perm in enumerate(n_perm)}\n",
    "feature_length = len(perm_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_bad_domains = min(MAX_DOMAINS, len(bad_domains))\n",
    "\n",
    "urls_features = np.zeros([len(good_urls) + num_bad_domains, feature_length], dtype=np.int8)\n",
    "\n",
    "for idx, good_url in enumerate(good_urls):\n",
    "    good_url = \"\".join([c for c in list(good_url) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(good_url)):\n",
    "        urls_features[idx, perm_lookup[gram]] += 1\n",
    "        \n",
    "for idx, bad_url in enumerate(bad_domains):\n",
    "    if idx >= MAX_DOMAINS:\n",
    "        break\n",
    "    bad_url = \"\".join([c for c in list(bad_url) if c in char_list])\n",
    "    for gram in list(ngram_gen._split(bad_url)):\n",
    "        urls_features[idx + len(good_urls), perm_lookup[gram]] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [\"good\" for i in range(len(good_urls))] + [\"bad\" for i in range(num_bad_domains)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(urls_features, y, test_size=0.25)\n",
    "\n",
    "del(urls_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.076 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.79      0.92      0.85     12482\n",
      "        bad       0.90      0.76      0.83     12518\n",
      "\n",
      "avg / total       0.85      0.84      0.84     25000\n",
      "\n",
      "CPU times: user 320 ms, sys: 84 ms, total: 404 ms\n",
      "Wall time: 405 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = MultinomialNB(alpha=150)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.49199999999999 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.85      0.98      0.91     12482\n",
      "        bad       0.97      0.83      0.90     12518\n",
      "\n",
      "avg / total       0.91      0.90      0.90     25000\n",
      "\n",
      "CPU times: user 3min 18s, sys: 316 ms, total: 3min 19s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=40, n_jobs=8)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.02799999999999 %\n",
      "Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       good       0.83      0.98      0.90     12482\n",
      "        bad       0.97      0.80      0.88     12518\n",
      "\n",
      "avg / total       0.90      0.89      0.89     25000\n",
      "\n",
      "CPU times: user 2h 18min 36s, sys: 480 ms, total: 2h 18min 37s\n",
      "Wall time: 2h 18min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {} %\".format(accuracy_score(y_test, y_predicted)*100))\n",
    "print(\"Classification Report \\n {}\".format(classification_report(y_test, y_predicted, labels=[\"good\", \"bad\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
